{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "from functools import reduce\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the class Nim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nim:\n",
    "    def __init__(self, num_rows: int, k: int = None) -> None:\n",
    "        self._rows = [i*2+1 for i in range(num_rows)]\n",
    "        self._k = k\n",
    "\n",
    "    def nimming_remove(self, row: int, num_objects: int) -> None:\n",
    "        assert self._rows[row] >= num_objects\n",
    "        assert self._k is None or num_objects < self._k\n",
    "        self._rows[row] -= num_objects\n",
    "    \n",
    "    def nimming_add(self, row: int, num_objects: int) -> None:\n",
    "        self._rows[row] += num_objects\n",
    "\n",
    "    def goal(self) -> bool:\n",
    "        # Check if someone has won\n",
    "        return sum(self._rows) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPlayer:\n",
    "    def __init__(self) -> None:\n",
    "        self._moves = []\n",
    "        self._nMoves = 0\n",
    "    \n",
    "    def play(self, nim: Nim) -> None:\n",
    "        # Chose a random row and a random number of pieces\n",
    "        x = random.randint(0, len(nim._rows)-1)\n",
    "        while nim._rows[x] == 0:\n",
    "            x = random.randint(0, len(nim._rows)-1)\n",
    "        y = random.randint(1, nim._rows[x])\n",
    "        # Update the attributes\n",
    "        self._nMoves += 1\n",
    "        self._moves.append(f\"{y} items from row {x}\")\n",
    "        nim.nimming_remove(x, y)\n",
    "        \n",
    "    def printSolution(self) -> None:\n",
    "        logging.info(f\" Random player won, moves have been: {self._moves}; total: {self._nMoves}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task3.1: An agent using fixed rules based on nim-sum (i.e., an expert system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_state(nrow: int, num_objects: int, nim: Nim) -> bool:\n",
    "    # Verify of the new game state is useful for the player with the nim sum rule\n",
    "    xor = reduce(lambda a, b: a ^ b, nim._rows)\n",
    "    if xor == 0:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expert System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpertSystem:\n",
    "    def __init__(self) -> None:\n",
    "        self._nMoves = 0\n",
    "        self._moves = []\n",
    "\n",
    "    def printSolution(self) -> None:\n",
    "        logging.info(f\" Expert system won, moves have been: {self._moves}; total: {self._nMoves}\")\n",
    "    \n",
    "    def play(self, nim: Nim) -> None:\n",
    "        x = 0\n",
    "        y = 0\n",
    "        valid = False\n",
    "        flag = True\n",
    "        not_found = False\n",
    "        x_already_taken = []\n",
    "        x_can_be_taken = [n for n,row in enumerate(nim._rows) if row!=0]\n",
    "        # Chose a random row and a random number of pieces\n",
    "        while flag:\n",
    "            x = random.randint(0, len(nim._rows)-1)\n",
    "            # If I have already tried the row x\n",
    "            while x in x_already_taken or x not in x_can_be_taken:\n",
    "                if len(x_already_taken) == len(x_can_be_taken):\n",
    "                    # All rows have already been tried\n",
    "                    flag = False\n",
    "                    not_found = True\n",
    "                    break\n",
    "                x = random.randint(0, len(nim._rows)-1)\n",
    "            x_already_taken.append(x)\n",
    "            y_already_taken = []\n",
    "            while flag == True and valid == False and len(y_already_taken) < nim._rows[x]:\n",
    "                y = random.randint(1, nim._rows[x])\n",
    "                if y not in y_already_taken:\n",
    "                    # If the deleting of y objects has not been already tried\n",
    "                    y_already_taken.append(y)\n",
    "                    nim.nimming_remove(x, y)\n",
    "                    valid = verify_state(x, y, nim)\n",
    "                    if valid == True:\n",
    "                        # Found a good move\n",
    "                        flag = False\n",
    "                    else:\n",
    "                        # Restore the game as before if the solution is useless for the player (search for another)\n",
    "                        nim.nimming_add(x, y)\n",
    "\n",
    "        if not_found:\n",
    "            # If no valid moves were found, do a random move\n",
    "            x = random.randint(0, len(nim._rows)-1)\n",
    "            while nim._rows[x] == 0:\n",
    "                x = random.randint(0, len(nim._rows)-1)\n",
    "            y = random.randint(1, nim._rows[x])\n",
    "            nim.nimming_remove(x, y)\n",
    "            \n",
    "        self._nMoves += 1\n",
    "        self._moves.append(f\"{y} items from row {x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game: Random vs Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nim = Nim(7)\n",
    "player1 = RandomPlayer()\n",
    "player2 = ExpertSystem()\n",
    "win = random.choice([1, 2])\n",
    "print(\"Start game: \", nim._rows)\n",
    "while nim.goal() == False:\n",
    "    if win == 1:\n",
    "        player1.play(nim)\n",
    "        print(\"Game after Random player played: \", nim._rows)\n",
    "        win = 2\n",
    "    else:\n",
    "        player2.play(nim)\n",
    "        print(\"Game after Expert system played: \", nim._rows)\n",
    "        win = 1\n",
    "if win == 1:\n",
    "    player1.printSolution()\n",
    "else:\n",
    "    player2.printSolution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task3.2: An agent using evolved rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task3.3: An agent using minmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinMax System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMaxSystem:\n",
    "    def __init__(self) -> None:\n",
    "        self._nmoves = 0\n",
    "        self._moves = []\n",
    "\n",
    "    def printSolution(self) -> None:\n",
    "        logging.info(f\" MinMax system won, moves have been: {self._moves}; total: {self._nmoves}\")\n",
    "\n",
    "    def minmax(self, nim: Nim, player: bool):\n",
    "        possible = [(r, o) for r, c in enumerate(nim._rows) for o in range(1, c + 1)]\n",
    "        # True player is the MinMax system, False player is the other player\n",
    "        if not possible:\n",
    "            if player:\n",
    "                # Win -> the MinMax system arrives with the table empty\n",
    "                return (None, 1, None)\n",
    "            else:\n",
    "                # No win -> the other player arrives with the table empty\n",
    "                return (None, -1, None)\n",
    "        evaluations = list()\n",
    "        for ply in possible:\n",
    "            nim.nimming_remove(ply[0], ply[1])\n",
    "            # Recursive call\n",
    "            _, val = self.minmax(nim, not player)\n",
    "            evaluations.append((ply, val))\n",
    "            # Restore the previous situation for another evaluation\n",
    "            nim.nimming_add(ply[0], ply[1])\n",
    "        return max(evaluations, key=lambda k: k[1])\n",
    "\n",
    "    def play(self, nim: Nim):\n",
    "        best_ply, _ = self.minmax(nim, player=True)\n",
    "        nim.nimming_remove(best_ply[0], best_ply[1])\n",
    "        self._nmoves += 1\n",
    "        self._moves.append(f\"{best_ply[1]} items from row {best_ply[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game: Random vs MinMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start game:  [1, 3, 5, 7]\n",
      "Game after Random player played:  [1, 2, 5, 7]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [9], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m     win \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 12\u001b[0m     player2\u001b[39m.\u001b[39;49mplay(nim)\n\u001b[0;32m     13\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mGame after MinMax system played: \u001b[39m\u001b[39m\"\u001b[39m, nim\u001b[39m.\u001b[39m_rows)\n\u001b[0;32m     14\u001b[0m     win \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "Cell \u001b[1;32mIn [8], line 32\u001b[0m, in \u001b[0;36mMinMaxSystem.play\u001b[1;34m(self, nim)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplay\u001b[39m(\u001b[39mself\u001b[39m, nim: Nim):\n\u001b[1;32m---> 32\u001b[0m     best_ply, _, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mminmax(nim, player\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, nmoves\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     33\u001b[0m     nim\u001b[39m.\u001b[39mnimming_remove(best_ply[\u001b[39m0\u001b[39m], best_ply[\u001b[39m1\u001b[39m])\n\u001b[0;32m     34\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_nmoves \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "Cell \u001b[1;32mIn [8], line 24\u001b[0m, in \u001b[0;36mMinMaxSystem.minmax\u001b[1;34m(self, nim, player, nmoves)\u001b[0m\n\u001b[0;32m     22\u001b[0m nmoves \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     23\u001b[0m \u001b[39m# Recursive call\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m _, val, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mminmax(nim, \u001b[39mnot\u001b[39;49;00m player, nmoves)\n\u001b[0;32m     25\u001b[0m evaluations\u001b[39m.\u001b[39mappend((ply, val, nmoves))\n\u001b[0;32m     26\u001b[0m \u001b[39m# Restore the previous situation for another evaluation\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [8], line 24\u001b[0m, in \u001b[0;36mMinMaxSystem.minmax\u001b[1;34m(self, nim, player, nmoves)\u001b[0m\n\u001b[0;32m     22\u001b[0m nmoves \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     23\u001b[0m \u001b[39m# Recursive call\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m _, val, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mminmax(nim, \u001b[39mnot\u001b[39;49;00m player, nmoves)\n\u001b[0;32m     25\u001b[0m evaluations\u001b[39m.\u001b[39mappend((ply, val, nmoves))\n\u001b[0;32m     26\u001b[0m \u001b[39m# Restore the previous situation for another evaluation\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping similar frames: MinMaxSystem.minmax at line 24 (7 times)]\u001b[0m\n",
      "Cell \u001b[1;32mIn [8], line 24\u001b[0m, in \u001b[0;36mMinMaxSystem.minmax\u001b[1;34m(self, nim, player, nmoves)\u001b[0m\n\u001b[0;32m     22\u001b[0m nmoves \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     23\u001b[0m \u001b[39m# Recursive call\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m _, val, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mminmax(nim, \u001b[39mnot\u001b[39;49;00m player, nmoves)\n\u001b[0;32m     25\u001b[0m evaluations\u001b[39m.\u001b[39mappend((ply, val, nmoves))\n\u001b[0;32m     26\u001b[0m \u001b[39m# Restore the previous situation for another evaluation\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [8], line 28\u001b[0m, in \u001b[0;36mMinMaxSystem.minmax\u001b[1;34m(self, nim, player, nmoves)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[39m# Restore the previous situation for another evaluation\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     nim\u001b[39m.\u001b[39mnimming_add(ply[\u001b[39m0\u001b[39m], ply[\u001b[39m1\u001b[39m])\n\u001b[1;32m---> 28\u001b[0m     nmoves \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     29\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mmax\u001b[39m(evaluations, key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m k: k[\u001b[39m1\u001b[39m])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nim = Nim(4)\n",
    "player1 = RandomPlayer()\n",
    "player2 = MinMaxSystem()\n",
    "win = random.choice([1])\n",
    "print(\"Start game: \", nim._rows)\n",
    "while nim.goal() == False:\n",
    "    if win == 1:\n",
    "        player1.play(nim)\n",
    "        print(\"Game after Random player played: \", nim._rows)\n",
    "        win = 2\n",
    "    else:\n",
    "        player2.play(nim)\n",
    "        print(\"Game after MinMax system played: \", nim._rows)\n",
    "        win = 1\n",
    "if win == 1:\n",
    "    player1.printSolution()\n",
    "else:\n",
    "    player2.printSolution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task3.4: An agent using reinforcement learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "98b7f83ea1f6816592be813d0a9257f9f3e2d15bf5c320e3b781c72faa584dce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
